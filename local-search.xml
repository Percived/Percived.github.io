<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>操作系统-硬件结构</title>
    <link href="/2025/04/13/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F-%E7%A1%AC%E4%BB%B6%E7%BB%93%E6%9E%84/"/>
    <url>/2025/04/13/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F-%E7%A1%AC%E4%BB%B6%E7%BB%93%E6%9E%84/</url>
    
    <content type="html"><![CDATA[<h1 id="操作系统"><a href="#操作系统" class="headerlink" title="操作系统"></a>操作系统</h1><h2 id="硬件结构"><a href="#硬件结构" class="headerlink" title="硬件结构"></a>硬件结构</h2><h3 id="64位相比32位CPU的优势在哪？64位CPU的计算性能一定比32位CPU高很多吗？"><a href="#64位相比32位CPU的优势在哪？64位CPU的计算性能一定比32位CPU高很多吗？" class="headerlink" title="64位相比32位CPU的优势在哪？64位CPU的计算性能一定比32位CPU高很多吗？"></a>64位相比32位CPU的优势在哪？64位CPU的计算性能一定比32位CPU高很多吗？</h3><ul><li>64位cpu可以一次计算超过32位的数字，32位cpu要计算超过32位数字要分多步骤进行计算。</li><li>64位cpu的地址访问空间是48位，32位只有32位，所以64位cpu可以分配更大的物理内存空间。32位寻址空间4g，48位是128T</li></ul><h3 id="存储器的层次结构"><a href="#存储器的层次结构" class="headerlink" title="存储器的层次结构"></a>存储器的层次结构</h3><p>我们想象中一个场景，大学期末准备考试了，你前去图书馆临时抱佛脚。那么，在看书的时候，我们的大脑会思考问题，也会记忆知识点，另外我们通常也会把常用的书放在自己的桌子上，当我们要找一本不常用的书，则会去图书馆的书架找。</p><p>就是这么一个小小的场景，已经把计算机的存储结构基本都涵盖了。</p><p>我们可以把 CPU 比喻成我们的大脑，大脑正在思考的东西，就好比 CPU 中的<strong>寄存器</strong>，处理速度是最快的，但是能存储的数据也是最少的，毕竟我们也不能一下同时思考太多的事情，除非你练过。</p><p>我们大脑中的记忆，就好比 <strong>CPU Cache</strong>，中文称为 CPU 高速缓存，处理速度相比寄存器慢了一点，但是能存储的数据也稍微多了一些。</p><p>CPU Cache 通常会分为 <strong>L1、L2、L3 三层</strong>，其中 L1 Cache 通常分成「数据缓存」和「指令缓存」，L1 是距离 CPU 最近的，因此它比 L2、L3 的读写速度都快、存储空间都小。我们大脑中短期记忆，就好比 L1 Cache，而长期记忆就好比 L2&#x2F;L3 Cache。</p><p>寄存器和 CPU Cache 都是在 CPU 内部，跟 CPU 挨着很近，因此它们的读写速度都相当的快，但是能存储的数据很少，毕竟 CPU 就这么丁点大。</p><p>知道 CPU 内部的存储器的层次分布，我们放眼看看 CPU 外部的存储器。</p><p>当我们大脑记忆中没有资料的时候，可以从书桌或书架上拿书来阅读，那我们桌子上的书，就好比<strong>内存</strong>，我们虽然可以一伸手就可以拿到，但读写速度肯定远慢于寄存器，那图书馆书架上的书，就好比<strong>硬盘</strong>，能存储的数据非常大，但是读写速度相比内存差好几个数量级，更别说跟寄存器的差距了。</p><p><img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E5%AD%98%E5%82%A8%E7%BB%93%E6%9E%84/%E5%AD%A6%E4%B9%A0%E4%B8%8E%E5%AD%98%E5%82%A8%E5%B1%82%E6%AC%A1%E5%85%B3%E7%B3%BB.png" alt="img"></p><p><img src="C:\Users\阿星\AppData\Roaming\Typora\typora-user-images\image-20250413143318567.png" alt="image-20250413143318567"></p><h3 id="如何写出让-CPU-跑得更快的代码？"><a href="#如何写出让-CPU-跑得更快的代码？" class="headerlink" title="如何写出让 CPU 跑得更快的代码？"></a>如何写出让 CPU 跑得更快的代码？</h3><p>CPU三级缓存</p><p>L1Cache通常会分为指令缓存和数据缓存</p><p>L1Cache和L2Cache是每个CPU核心独有的，而L3Cache是多个CPU核心共享的</p><p><img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E5%AD%98%E5%82%A8%E7%BB%93%E6%9E%84/CPU-Cache.png" alt="img"></p><p>CPU Cache是由多个Cache Line组成的</p><p><img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost3@main/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/CPU%E7%BC%93%E5%AD%98%E4%B8%80%E8%87%B4%E6%80%A7/Cache%E7%9A%84%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84.png" alt="img"></p><p>内存到CPU Cache的映射方式</p><ul><li>直接映射：取模运算，内存块对应的缓存块是固定的</li><li>组相联</li><li>全相联</li></ul><p><strong>直接映射</strong></p><p>为了区别不同的内存块，在对应的 CPU Cache Line 中我们还会存储一个<strong>组标记（Tag）</strong>。这个组标记会记录当前 CPU Cache Line 中存储的数据对应的内存块，我们可以用这个组标记来区分不同的内存块。</p><p>除了组标记信息外，CPU Cache Line 还有两个信息：</p><ul><li>一个是，从内存加载过来的实际存放<strong>数据（*Data*）</strong>。</li><li>另一个是，<strong>有效位（*Valid bit*）</strong>，它是用来标记对应的 CPU Cache Line 中的数据是否是有效的，如果有效位是 0，无论 CPU Cache Line 中是否有数据，CPU 都会直接访问内存，重新加载数据。</li></ul><p>CPU 在从 CPU Cache 读取数据的时候，并不是读取 CPU Cache Line 中的整个数据块，而是读取 CPU 所需要的一个数据片段，这样的数据统称为一个<strong>字（*Word*）</strong>。那怎么在对应的 CPU Cache Line 中数据块中找到所需的字呢？答案是，需要一个<strong>偏移量（Offset）</strong>。</p><p>因此，一个内存的访问地址，包括<strong>组标记、CPU Cache Line 索引、偏移量</strong>这三种信息，于是 CPU 就能通过这些信息，在 CPU Cache 中找到缓存的数据。而对于 CPU Cache 里的数据结构，则是由<strong>索引 + 有效位 + 组标记 + 数据块</strong>组成。</p><p><img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost3@main/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/CPU%E7%BC%93%E5%AD%98/%E7%9B%B4%E6%8E%A5Cache%E6%98%A0%E5%B0%84.png" alt="img"></p><p>如果内存中的数据已经在 CPU Cache 中了，那 CPU 访问一个内存地址的时候，会经历这 4 个步骤：</p><ol><li>根据内存地址中索引信息，计算在 CPU Cache 中的索引，也就是找出对应的 CPU Cache Line 的地址；</li><li>找到对应 CPU Cache Line 后，判断 CPU Cache Line 中的有效位，确认 CPU Cache Line 中数据是否是有效的，如果是无效的，CPU 就会直接访问内存，并重新加载数据，如果数据有效，则往下执行；</li><li>对比内存地址中组标记和 CPU Cache Line 中的组标记，确认 CPU Cache Line 中的数据是我们要访问的内存数据，如果不是的话，CPU 就会直接访问内存，并重新加载数据，如果是的话，则往下执行；</li><li>根据内存地址中偏移量信息，从 CPU Cache Line 的数据块中，读取对应的字。</li></ol><h3 id="让CPU跑得更快-提高缓存命中率"><a href="#让CPU跑得更快-提高缓存命中率" class="headerlink" title="让CPU跑得更快&#x3D;提高缓存命中率"></a>让CPU跑得更快&#x3D;提高缓存命中率</h3><h4 id="提升数据缓存的命中率"><a href="#提升数据缓存的命中率" class="headerlink" title="提升数据缓存的命中率"></a>提升数据缓存的命中率</h4><p>数据在读取时保持顺序访问，一次读取一个Cache Line的大小，它表示 <strong>CPU Cache 一次性能加载数据的大小</strong>，可以在 Linux 里通过 <code>coherency_line_size</code> 配置查看 它的大小，通常是 64 个字节。</p><p>当 CPU 访问内存数据时，如果数据不在 CPU Cache 中，则会一次性会连续加载 64 字节大小的数据到 CPU Cache，那么当访问 <code>array[0][0]</code> 时，由于该元素不足 64 字节，于是就会往后<strong>顺序</strong>读取 <code>array[0][0]~array[0][15]</code> 到 CPU Cache 中。顺序访问的 <code>array[i][j]</code> 因为利用了这一特点，所以就会比跳跃式访问的 <code>array[j][i]</code> 要快。</p><p><strong>因此，遇到这种遍历数组的情况时，按照内存布局顺序访问，将可以有效的利用 CPU Cache 带来的好处，这样我们代码的性能就会得到很大的提升</strong></p><h4 id="提升指令缓存的命中率"><a href="#提升指令缓存的命中率" class="headerlink" title="提升指令缓存的命中率"></a>提升指令缓存的命中率</h4><p><strong>CPU分支预测器</strong></p><p>对于 if 条件语句，意味着此时至少可以选择跳转到两段不同的指令执行，也就是 if 还是 else 中的指令。那么，<strong>如果分支预测可以预测到接下来要执行 if 里的指令，还是 else 指令的话，就可以「提前」把这些指令放在指令缓存中，这样 CPU 可以直接从 Cache 读取到指令，于是执行速度就会很快</strong>。</p><p>当数组中的元素是随机的，分支预测就无法有效工作，而当数组元素都是是顺序的，分支预测器会动态地根据历史命中数据对未来进行预测，这样命中率就会很高。</p><p>因此，先排序再遍历速度会更快，这是因为排序之后，数字是从小到大的，那么前几次循环命中 <code>if &lt; 50</code> 的次数会比较多，于是分支预测就会缓存 <code>if</code> 里的 <code>array[i] = 0</code> 指令到 Cache 中，后续 CPU 执行该指令就只需要从 Cache 读取就好了。</p><h3 id="多核CPU如何提升缓存命中率"><a href="#多核CPU如何提升缓存命中率" class="headerlink" title="多核CPU如何提升缓存命中率"></a>多核CPU如何提升缓存命中率</h3><p>前面提到，L1，L2缓存是CPU核心独有的，而线程运行时只会在一个核心上运行，操作该核心的缓存，如果发生线程切换，线程再次运行后在一个新核心上运行，那么上次运行时的L1，L2缓存就无法命中了，所以我们尽量让线程在一个核心上运行，绑定到该核心上。在 Linux 上提供了 <code>sched_setaffinity</code> 方法，来实现将线程绑定到某个 CPU 核心这一功能。</p><p><img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost3@main/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/CPU%E7%BC%93%E5%AD%98/sched_setaffinity.png" alt="img"></p><h3 id="CPU缓存一致性"><a href="#CPU缓存一致性" class="headerlink" title="CPU缓存一致性"></a>CPU缓存一致性</h3><h4 id="数据写入"><a href="#数据写入" class="headerlink" title="数据写入"></a>数据写入</h4><p>我们当然期望 CPU 读取数据的时候，都是尽可能地从 CPU Cache 中读取，而不是每一次都要从内存中获取数据。</p><p>事实上，数据不光是只有读操作，还有写操作，那么如果数据写入 Cache 之后，内存与 Cache 相对应的数据将会不同，这种情况下 Cache 和内存数据都不一致了，于是我们肯定是要把 Cache 中的数据同步到内存里的。</p><p>问题来了，那在什么时机才把 Cache 中的数据写回到内存呢？</p><ul><li>写直达（<em>Write Through</em>）</li><li>写回（<em>Write Back</em>）</li></ul><h5 id="写直达"><a href="#写直达" class="headerlink" title="写直达"></a>写直达</h5><p>在这个方法里，写入前会先判断数据是否已经在 CPU Cache 里面了：</p><ul><li>如果数据已经在 Cache 里面，先将数据更新到 Cache 里面，再写入到内存里面；</li><li>如果数据没有在 Cache 里面，就直接把数据更新到内存里面。</li></ul><p>写直达法很直观，也很简单，但是问题明显，无论数据在不在 Cache 里面，每次写操作都会写回到内存，这样写操作将会花费大量的时间，无疑性能会受到很大的影响</p><h5 id="写回"><a href="#写回" class="headerlink" title="写回"></a>写回</h5><p>既然写直达由于每次写操作都会把数据写回到内存，而导致影响性能，于是为了要减少数据写回内存的频率，就出现了<strong>写回（*Write Back*）的方法</strong>。</p><p>在写回机制中，<strong>当发生写操作时，新的数据仅仅被写入 Cache Block 里，只有当修改过的 Cache Block「被替换」时才需要写到内存中</strong>，减少了数据写回内存的频率，这样便可以提高系统的性能。</p><p><img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost3@main/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/CPU%E7%BC%93%E5%AD%98%E4%B8%80%E8%87%B4%E6%80%A7/%E5%86%99%E5%9B%9E1.png" alt="img"></p><h4 id="缓存一致性问题"><a href="#缓存一致性问题" class="headerlink" title="缓存一致性问题"></a>缓存一致性问题</h4><p>现在 CPU 都是多核的，由于 L1&#x2F;L2 Cache 是多个核心各自独有的，那么会带来多核心的<strong>缓存一致性（*Cache Coherence*）</strong> 的问题，如果不能保证缓存一致性的问题，就可能造成结果错误。</p><p>那缓存一致性的问题具体是怎么发生的呢？我们以一个含有两个核心的 CPU 作为例子看一看。</p><p>假设 A 号核心和 B 号核心同时运行两个线程，都操作共同的变量 i（初始值为 0 ）。</p><p><img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost3@main/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/CPU%E7%BC%93%E5%AD%98%E4%B8%80%E8%87%B4%E6%80%A7/%E7%BC%93%E5%AD%98%E4%B8%80%E8%87%B4%E6%80%A7%E9%97%AE%E9%A2%98%E4%BE%8B%E5%AD%902.png" alt="img"></p><p>这时如果 A 号核心执行了 <code>i++</code> 语句的时候，为了考虑性能，使用了我们前面所说的写回策略，先把值为 <code>1</code> 的执行结果写入到 L1&#x2F;L2 Cache 中，然后把 L1&#x2F;L2 Cache 中对应的 Block 标记为脏的，这个时候数据其实没有被同步到内存中的，因为写回策略，只有在 A 号核心中的这个 Cache Block 要被替换的时候，数据才会写入到内存里。</p><p>如果这时旁边的 B 号核心尝试从内存读取 i 变量的值，则读到的将会是错误的值，因为刚才 A 号核心更新 i 值还没写入到内存中，内存中的值还依然是 0。<strong>这个就是所谓的缓存一致性问题，A 号核心和 B 号核心的缓存，在这个时候是不一致，从而会导致执行结果的错误。</strong></p><h4 id="MESI协议"><a href="#MESI协议" class="headerlink" title="MESI协议"></a>MESI协议</h4><p>MESI 协议其实是 4 个状态单词的开头字母缩写，分别是：</p><ul><li><em>Modified</em>，已修改</li><li><em>Exclusive</em>，独占</li><li><em>Shared</em>，共享</li><li><em>Invalidated</em>，已失效</li></ul><p>这四个状态来标记 Cache Line 四个不同的状态。</p><p>「已修改」状态就是我们前面提到的脏标记，代表该 Cache Block 上的数据已经被更新过，但是还没有写到内存里。而「已失效」状态，表示的是这个 Cache Block 里的数据已经失效了，不可以读取该状态的数据。</p><p>「独占」和「共享」状态都代表 Cache Block 里的数据是干净的，也就是说，这个时候 Cache Block 里的数据和内存里面的数据是一致性的。</p><p>「独占」和「共享」的差别在于，独占状态的时候，数据只存储在一个 CPU 核心的 Cache 里，而其他 CPU 核心的 Cache 没有该数据。这个时候，如果要向独占的 Cache 写数据，就可以直接自由地写入，而不需要通知其他 CPU 核心，因为只有你这有这个数据，就不存在缓存一致性的问题了，于是就可以随便操作该数据。</p><p>另外，在「独占」状态下的数据，如果有其他核心从内存读取了相同的数据到各自的 Cache ，那么这个时候，独占状态下的数据就会变成共享状态。</p><p>那么，「共享」状态代表着相同的数据在多个 CPU 核心的 Cache 里都有，所以当我们要更新 Cache 里面的数据的时候，不能直接修改，而是要先向所有的其他 CPU 核心广播一个请求，要求先把其他核心的 Cache 中对应的 Cache Line 标记为「无效」状态，然后再更新当前 Cache 里面的数据。</p><p><img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost3@main/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/CPU%E7%BC%93%E5%AD%98%E4%B8%80%E8%87%B4%E6%80%A7/MESI%E5%8D%8F%E8%AE%AE.png" alt="img"></p><h3 id="CPU伪共享问题"><a href="#CPU伪共享问题" class="headerlink" title="CPU伪共享问题"></a>CPU伪共享问题</h3><p>我们知道CPU核心有自己独有的L1，L2缓存，而且有MESI协议存在，如果两个线程在两个不同的核心上并行，分别读取A,B两个变量，这两个变量在内存中时连续存储的，位于一个内存块中，归属于一个Cache Line，考虑当两个线程交替修改A，B的值时，根据MESI协议，它们的Cache Line状态会被不停更改，导致核心不停去内存读取数据。</p><p>因此，这种因为多个线程同时读写同一个 Cache Line 的不同变量时，而导致 CPU Cache 失效的现象称为<strong>伪共享（*False Sharing*）</strong></p><h4 id="避免伪共享"><a href="#避免伪共享" class="headerlink" title="避免伪共享"></a>避免伪共享</h4><p>即避免多线程的热点数据位于同一个Cache Line中</p><p>在 Linux 内核中存在 <code>__cacheline_aligned_in_smp</code> 宏定义，是用于解决伪共享的问题。</p><p><img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost3@main/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/CPU%E4%BC%AA%E5%85%B1%E4%BA%AB/__cacheline_aligned.png" alt="img"></p><p>从上面的宏定义，我们可以看到：</p><ul><li>如果在多核（MP）系统里，该宏定义是 <code>__cacheline_aligned</code>，也就是 Cache Line 的大小；</li><li>而如果在单核系统里，该宏定义是空的；</li></ul><p>因此，针对在同一个 Cache Line 中的共享的数据，如果在多核之间竞争比较严重，为了防止伪共享现象的发生，可以采用上面的宏定义使得变量在 Cache Line 里是对齐的</p><p>即内存对齐填充</p><h3 id="中断处理"><a href="#中断处理" class="headerlink" title="中断处理"></a>中断处理</h3><h4 id="硬件中断"><a href="#硬件中断" class="headerlink" title="硬件中断"></a>硬件中断</h4><p>在计算机中，中断是系统用来响应硬件设备请求的一种机制，操作系统收到硬件的中断请求，会打断正在执行的进程，然后调用内核中的中断处理程序来响应请求。</p><p>那 Linux 系统<strong>为了解决中断处理程序执行过长和中断丢失的问题，将中断过程分成了两个阶段，分别是「上半部和下半部分」</strong>。</p><ul><li><strong>上半部用来快速处理中断</strong>，一般会暂时关闭中断请求，主要负责处理跟硬件紧密相关或者时间敏感的事情。</li><li><strong>下半部用来延迟处理上半部未完成的工作</strong>，一般以「内核线程」的方式运行。</li></ul><p>举一个计算机中的例子，常见的网卡接收网络包的例子。</p><p>网卡收到网络包后，通过 DMA 方式将接收到的数据写入内存，接着会通过<strong>硬件中断</strong>通知内核有新的数据到了，于是内核就会调用对应的中断处理程序来处理该事件，这个事件的处理也是会分成上半部和下半部。</p><p>上部分要做的事情很少，会先禁止网卡中断，避免频繁硬中断，而降低内核的工作效率。接着，内核会触发一个<strong>软中断</strong>，把一些处理比较耗时且复杂的事情，交给「软中断处理程序」去做，也就是中断的下半部，其主要是需要从内存中找到网络数据，再按照网络协议栈，对网络数据进行逐层解析和处理，最后把数据送给应用程序。</p><p>所以，中断处理程序的上部分和下半部可以理解为：</p><ul><li><strong>上半部直接处理硬件请求，也就是硬中断</strong>，主要是负责耗时短的工作，特点是快速执行；</li><li><strong>下半部是由内核触发，也就说软中断</strong>，主要是负责上半部未完成的工作，通常都是耗时比较长的事情，特点是延迟执行；</li></ul><p>还有一个区别，硬中断（上半部）是会打断 CPU 正在执行的任务，然后立即执行中断处理程序，而软中断（下半部）是以内核线程的方式执行，并且每一个 CPU 都对应一个软中断内核线程，名字通常为「ksoftirqd&#x2F;CPU 编号」，比如 0 号 CPU 对应的软中断内核线程的名字是 <code>ksoftirqd/0</code></p><p>不过，软中断不只是包括硬件设备中断处理程序的下半部，一些内核自定义事件也属于软中断，比如内核调度等、RCU 锁（内核里常用的一种锁）等。</p><h4 id="软中断和软件中断的区别"><a href="#软中断和软件中断的区别" class="headerlink" title="软中断和软件中断的区别"></a>软中断和软件中断的区别</h4><h5 id="软中断（Softirq）"><a href="#软中断（Softirq）" class="headerlink" title="软中断（Softirq）"></a>软中断（Softirq）</h5><p><strong>定义</strong><br>内核机制：软中断是 Linux 内核中一种延迟处理机制，用于在中断上下文之外执行耗时任务。<br>触发方式：由内核代码（如设备驱动或网络协议栈）通过 raise_softirq() 主动触发。<br>处理时机：在以下场景中处理：<br>硬中断处理程序返回时。<br>内核调度器（如 ksoftirqd 线程）主动轮询。<br><strong>特点</strong><br>异步延迟执行：不会立即响应，而是由内核选择合适时机批量处理。<br>无上下文切换：运行在内核态，直接使用当前 CPU 上下文。<br>固定类型：Linux 内核预定义了有限的软中断类型（如 NET_RX_SOFTIRQ、TIMER_SOFTIRQ）。<br><strong>典型应用</strong><br>网络数据包的上层处理（如 TCP&#x2F;IP 拆包）。<br>定时器回调（如 jiffies 更新）。<br>块设备 I&#x2F;O 完成后的后续操作。<br><strong>示例流程</strong></p><ul><li>网卡收到数据包 → 触发硬中断。</li><li>硬中断处理程序将数据拷贝到内存 → 触发 <code>NET_RX_SOFTIRQ</code>。</li><li>内核在稍后处理软中断 → 执行网络协议栈逻辑。</li></ul><ol start="2"><li><h5 id="软件中断（Software-Interrupt）"><a href="#软件中断（Software-Interrupt）" class="headerlink" title="软件中断（Software Interrupt）"></a>软件中断（Software Interrupt）</h5></li></ol><p><strong>定义</strong><br>CPU 指令触发：通过执行特定指令（如 x86 的 INT 0x80、ARM 的 SWI）主动发起的中断。<br>同步调用：由程序显式触发，立即跳转到预设的中断处理程序。<br>用途：主要用于系统调用（System Call） 或 调试异常。<br><strong>特点</strong><br>同步执行：调用后立即处理，类似函数调用。<br>用户态到内核态切换：用于实现特权级切换（如用户程序调用内核功能）。<br>动态注册：处理程序可通过内核 API 动态注册（如 syscall_table）。<br><strong>典型应用</strong><br>系统调用（如 read()、write()）。<br>调试断点（如 int 3）。<br>虚拟化场景（如 VM 退出到宿主机）。<br><strong>示例流程</strong></p><ul><li><p>用户程序调用 <code>write()</code> → 触发 <code>INT 0x80</code> 或 <code>syscall</code> 指令。</p></li><li><p>CPU 切换到内核态 → 执行系统调用处理程序。</p></li><li><p>内核完成文件写入 → 返回用户程序。</p></li></ul>]]></content>
    
    
    
  </entry>
  
  
  
  
</search>
